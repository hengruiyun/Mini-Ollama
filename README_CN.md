# 迷你 Ollama （AI智能）

## 致谢

**特别感谢 [Ollama 团队](https://github.com/ollama/ollama) 创造了这个令人惊叹的 AI 平台！**

本项目建立在 Ollama 的基础之上，这是一个令人难以置信的开源平台，让每个人都能在本地运行 AI 模型。没有Ollama 团队的创新工作，迷你 Ollama 就不会存在。我们感谢他们对 AI 技术的奉献，以及让全世界的开发者和用户都能使用这项技术。

---

一个革命性的轻量级GUI，可完全基于CPU运行强大的AI模型 - 无需GPU！适合任何电脑，从轻薄笔记本到企业电脑。

<img width="1040" height="807" alt="mO-1" src="https://github.com/user-attachments/assets/cfe28cac-fcea-41b1-b088-8079db617f27" />


## 核心特性

### **革命性的纯CPU AI - 无需GPU！**
- **零GPU依赖**: 仅使用CPU运行强大的大语言模型 - 无需昂贵的显卡！
- **通用兼容性**: 适用于任何电脑 - 从10年前的笔记本到现代电脑 
- **即装即用**: 无需CUDA驱动，无需GPU兼容性检查 - 安装即可运行

### **大语言模型民主化**
- **人人可用的AI**: 为每个电脑用户带来企业级AI能力
- **无硬件门槛**: 学生、研究人员和开发者无论硬件预算如何都能体验AI
- **小企业就绪**: 让小企业无需基础设施投资即可启用AI功能
- **离线智能**: 完整AI功能无需互联网依赖或云服务费用
- **隐私优先**: 您的数据永不离开您的电脑 - 完全隐私和安全

### **完整的 Ollama 管理**
- **开机自启配置**: 自动在开机时启动 Ollama 服务
- **模型管理**: 下载、删除和管理 AI 模型
- **聊天界面**: 内置聊天界面用于测试模型
- **双语支持**: 自动语言检测（英文/中文）

### **用户友好设计**
- **现代 GUI**: 基于 Tkinter 构建的清洁、直观界面
- **实时状态**: 实时服务状态和连接监控
- **进度跟踪**: 模型下载的可视化进度条


## 性能数据 - 纯CPU AI运行实证！

| 指标 | 数值 | 描述 |
|------|------|------|
| **GPU需求** | **零** | **无需显卡 - 集成显卡即可运行！** |
| **CPU支持** | 任意x64 | Intel、AMD，甚至较老处理器都能完美运行 |
| **启动时间** | <3秒 | 无需漫长GPU初始化，即时访问AI |
| **模型大小** | 0.5-1.5GB | 为CPU推理优化的紧凑而强大的模型 |
| **最低内存** | 4GB | 推荐用于大模型流畅运行 |


<img width="1040" height="807" alt="mO-2c" src="https://github.com/user-attachments/assets/01e27887-b4af-4b96-b1a0-dadceebe1291" />


## 使用场景 - 纯CPU AI革命

### **适合所有没有昂贵硬件的用户:**
- **预算开发者**: 无需投资2万元GPU即可构建AI应用
- **学生和研究人员**: 在大学/个人笔记本上访问强大语言模型
- **小企业**: 经济实惠地实现AI客服、内容生成和自动化
- **内容创作者**: 仅使用现有电脑生成文章、代码和创意内容
- **隐私重视用户**: 在享受AI助手的同时保持敏感数据本地化
- **教育机构**: 无需昂贵实验设备即可教授AI概念
- **远程工作者**: 随处可用的AI生产力工具，甚至离线可用
- **业余程序员**: 在周末项目中体验尖端AI
- **旧硬件用户**: 为老旧电脑注入新的AI活力
- **安静环境**: 图书馆、共享办公室、卧室 - 无噪音GPU散热

### **现实世界的纯CPU AI应用:**
- **代码助手**: 获得编程帮助、调试代码、解释算法
- **写作伙伴**: 生成文章、邮件、创意写作、文档
- **语言翻译**: 完全离线的多语言文本翻译
- **研究助手**: 总结文档、回答问题、分析数据
- **学习导师**: 获得复杂主题解释、练习对话
- **业务自动化**: 生成报告、分析反馈、创建提案

### **不推荐用于:**
- 大模型实时视频处理
- 从头训练新模型（仅推理）
- 需要亚秒级响应的极时敏应用


## 快速开始

### 前置要求 - 无需GPU！
- **CPU**: 任意64位处理器（Intel/AMD）- 甚至较老型号都能运行！
- **GPU**: **不需要** - 集成显卡即可满足需求
- **操作系统**: Windows 10/11 (64位)
- **运行环境**: Python 3.10 或更高版本
- **内存**: 4GB+（推荐8GB用于更大模型）
- **存储**: 5GB+ 可用磁盘空间（用于AI模型）
- **显卡**: 基础集成显卡（无需独立GPU）
- **电源**: 标准笔记本/台式机电源（无需高功率PSU）

### 安装

1. **下载 迷你 Ollama一键安装包**
   ```bash
   MiniOllamaSetup.exe
   ```
2. **运行应用程序**
   ```bash
   MiniOllama,exe
   ```

### 首次设置

1. **下载您的第一个模型**
   - 转到"模型管理"选项卡
   - 选择一个轻量级模型 (例如 `qwen3: 0.6b - 0.5GB)
   - 点击"下载模型"
   - 等待下载完成

2. **测试模型**
   - 切换到"聊天界面"选项卡
   - 选择您下载的模型
   - 输入消息并点击"发送"

3. **配置自动启动** (可选)
   - 切换到"开机启动"选项卡
   - 启用"开机自动启动 Ollama 服务"



## 推荐模型

| 模型 | 大小 | 使用场景 | 性能 |
|------|------|----------|------|
| `tinyllama:1.1b` | 0.6GB | 基础聊天、学习 | 快速、低资源 |
| `qwen3:0.6b` | 0.5GB | 轻量级任务 | 非常快 |
| `gemma3:1b` | 0.8GB | 通用目的 | 平衡 |
| `deepseek-r1:1.5b` | 1.1GB | 代码辅助 | 良好推理 |
| `llama3.2:1b` | 1.3GB | 高级聊天 | 最佳质量 |


## 配置

### 环境变量
应用程序管理这些 Ollama 环境变量:

- **OLLAMA_HOST**: 服务器地址 (默认: localhost)
- **OLLAMA_PORT**: 端口号 (默认: 11434)
- **OLLAMA_MODELS**: 模型存储路径 (默认: ~/.ollama/models)
- **OLLAMA_KEEP_ALIVE**: 模型保持活跃时间 (默认: 5m)


## 许可证

本项目采用 Apache 2.0 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情。


## 支持

### 获取帮助
- Email：267278466@qq.com

---

**MiniOllama** - 让 AI 在任何 Windows 电脑上都能使用！ 🚀

*专为开发、学习和低资源环境而设计。* 
